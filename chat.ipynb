{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d74bc17",
   "metadata": {},
   "source": [
    "# Document Question Answering Chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "51306115",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import RetrievalQA, ConversationalRetrievalChain\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9107c79",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import textwrap\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv())\n",
    "\n",
    "open.api_key = os.environ['OPENAI_API_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "15f72462",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_question_with_context(qa, question, chat_history):\n",
    "    result = qa({\"question\": question, \"chat_history\": chat_history})\n",
    "    print(f\"\\n{textwrap.fill(result['answer'])}\\n\")\n",
    "    chat_history = [(question, result[\"answer\"])]\n",
    "    return chat_history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b6ecc11",
   "metadata": {},
   "source": [
    "## Initialize ChromaDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0497c4ea",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "199\n"
     ]
    }
   ],
   "source": [
    "embeddings = OpenAIEmbeddings()\n",
    "persist_directory = 'chroma/'\n",
    "vectordb = Chroma(persist_directory=persist_directory, embedding_function=embeddings)\n",
    "print(vectordb._collection.count())\n",
    "vectordb.persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f3303af",
   "metadata": {},
   "source": [
    "## Create the chain and a helper function\n",
    "\n",
    "Initialize the chain we will use for question answering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ae360a7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "llm_model_name = \"gpt-3.5-turbo\"\n",
    "#llm_model_name = \"gpt-4\"\n",
    "llm = ChatOpenAI(model_name=llm_model_name, temperature=0)\n",
    "\n",
    "retriever = vectordb.as_retriever(search_type=\"similarity\", search_kwargs={\"k\":5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f38a229a",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUESTION_PROMPT = PromptTemplate.from_template(\"\"\"Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question.\n",
    "\n",
    "    Chat History:\n",
    "    {chat_history}\n",
    "    Follow Up Input: {question}\n",
    "    Standalone question:\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aa41b83f",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa = ConversationalRetrievalChain.from_llm(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    condense_question_prompt=QUESTION_PROMPT,\n",
    "    return_source_documents=True,\n",
    "    verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e079e4b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf8964a",
   "metadata": {},
   "source": [
    "## Ask questions!\n",
    "\n",
    "Now we can use the chain to ask questions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "566d0f37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "you:  What is the question generation task in natural language processing?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The question generation task in natural language processing involves\n",
      "training a machine to produce questions based on a given context and\n",
      "answer. The goal is to generate fluent and high-quality questions that\n",
      "are relevant to the context and can be answered correctly.\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "you:  Can you summarize the paper on that topic included in our data set?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The paper discusses recent advances in neural question generation,\n",
      "which is a task in natural language processing. The authors explore\n",
      "different models trained on various datasets, such as SQuAD, Natural\n",
      "Questions (NQ), Question Answering in Context (QuAC), and TriviaQA.\n",
      "They evaluate the models using metrics that account for semantic\n",
      "similarity and lexical similarity to measure the quality of the\n",
      "generated questions. The paper highlights the success of their models\n",
      "in producing fluent and high-quality questions.\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "you:  Who wrote that paper?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The authors of the paper included in the dataset are Liangming Pan,\n",
      "Wenqiang Lei, Tat-Seng Chua, and Min-Yen Kan.\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "you:  Are you sure about that?  I thought those people wrote a paper referenced in the paper you summarized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Based on the provided context, there is no information about whether\n",
      "the people mentioned in the context wrote a paper referenced in the\n",
      "summarized paper.\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "you:  I thought Richard Robbins was an author of the paper we are talking about.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "No, Richard Robbins is not mentioned as an author in the context\n",
      "provided.\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "you:  What the paper written for a class project?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Based on the provided context, there is no information to suggest that\n",
      "the paper was written for a class project.\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "you:  What does the paper say about semantic evaluation metrics?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The paper states that metrics that account for semantic similarity\n",
      "produce scores that are more reflective of successful question\n",
      "generation than those based on lexical similarity. It also mentions\n",
      "that semantic metrics are more robust indicators of success at\n",
      "question generation than lexical metrics. Additionally, the paper\n",
      "suggests that semantic metrics can be used together with lexical\n",
      "metrics for a more comprehensive evaluation of question quality.\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "you:  What training data did they use?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The paper used four source datasets for training the models: SQuAD,\n",
      "Natural Questions (NQ), Question Answering in Context (QuAC), and\n",
      "TriviaQA. Additionally, they created a blended dataset by combining\n",
      "the four source datasets and randomly shuffling the data. The models\n",
      "were trained on this shuffled blended dataset.\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "you:  Which model performed the best?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Based on the information provided, the model trained on the shuffled\n",
      "blended dataset had the highest performance.\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "you:  Did the models work well with the QuAC data?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The models struggled with the QuAC dataset in general, and the results\n",
      "were low compared to the best performance on other datasets.\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "you:  Why?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The reason for the models struggling with the QuAC dataset and\n",
      "achieving low results compared to other datasets is not explicitly\n",
      "mentioned in the given context.\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "you:  q\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    query = input('you: ')\n",
    "    if query == 'q':\n",
    "        break\n",
    "    chat_history = ask_question_with_context(qa, query, chat_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d29b75c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
