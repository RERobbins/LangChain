{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "d796c090",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "import sys\n",
    "import glob\n",
    "import tiktoken\n",
    "sys.path.append('../..')\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv())\n",
    "\n",
    "open.api_key = os.environ['OPENAI_API_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "86da37b8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader, TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.retrievers.self_query.base import SelfQueryRetriever\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.chains.query_constructor.base import AttributeInfo\n",
    "from langchain.retrievers.document_compressors import LLMChainExtractor\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "95838b15",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data/silverman-openai-complaint.pdf',\n",
       " 'data/doc-5.txt',\n",
       " 'data/The_Effect_of_Student_Teacher_Ratio_on_Truancy.pdf',\n",
       " 'data/doc-3.txt',\n",
       " 'data/Question_Generation.pdf',\n",
       " 'data/doc-2.txt',\n",
       " 'data/state_of_the_union.txt',\n",
       " 'data/fec_2016_EDA.v2.pdf',\n",
       " 'data/doc-4.txt',\n",
       " 'data/2023-08-01_Trump_Indictment.pdf',\n",
       " 'data/exploring-ggplot.pdf',\n",
       " 'data/doc-6.txt',\n",
       " 'data/doc-1.txt']"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glob.glob(\"data/*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "206eaecf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "loaders = [PyPDFLoader(pdf) for pdf in glob.glob(\"data/*.pdf\")] + \\\n",
    "          [TextLoader (txt) for txt in glob.glob(\"data/*.txt\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "b4b82373",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<langchain.document_loaders.pdf.PyPDFLoader at 0x7f973c26c400>,\n",
       " <langchain.document_loaders.pdf.PyPDFLoader at 0x7f9730b4ebf0>,\n",
       " <langchain.document_loaders.pdf.PyPDFLoader at 0x7f9724e527d0>,\n",
       " <langchain.document_loaders.pdf.PyPDFLoader at 0x7f9748357fa0>,\n",
       " <langchain.document_loaders.pdf.PyPDFLoader at 0x7f971ad53f40>,\n",
       " <langchain.document_loaders.pdf.PyPDFLoader at 0x7f971ad53550>,\n",
       " <langchain.document_loaders.text.TextLoader at 0x7f9730b4f850>,\n",
       " <langchain.document_loaders.text.TextLoader at 0x7f971ad52c50>,\n",
       " <langchain.document_loaders.text.TextLoader at 0x7f971ad52d70>,\n",
       " <langchain.document_loaders.text.TextLoader at 0x7f971ad52770>,\n",
       " <langchain.document_loaders.text.TextLoader at 0x7f971ad529b0>,\n",
       " <langchain.document_loaders.text.TextLoader at 0x7f971ad53100>,\n",
       " <langchain.document_loaders.text.TextLoader at 0x7f971ad52fe0>]"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "c89f3abb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "docs = []\n",
    "for loader in loaders:\n",
    "    docs.extend(loader.load())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "490ed614",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "123"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "13e845f6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 1500,\n",
    "    chunk_overlap = 150\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "42be0aa8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "splits = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "6f33be28",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "257"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "ff8e1dc7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "embedding = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "d76a6cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence1 = \"That movie was great\"\n",
    "sentence2 = \"That movie was amazing\"\n",
    "sentence3 = \"That film was awesome\"\n",
    "sentence4 = \"That movie was garbage\"\n",
    "sentence5 = \"Take out the garbage\"\n",
    "sentence6 = \"That show was bananas\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "f1b4adf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding1 = embedding.embed_query(sentence1)\n",
    "embedding2 = embedding.embed_query(sentence2)\n",
    "embedding3 = embedding.embed_query(sentence3)\n",
    "embedding4 = embedding.embed_query(sentence4)\n",
    "embedding5 = embedding.embed_query(sentence5)\n",
    "embedding6 = embedding.embed_query(sentence6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "bfdbe587",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "70d65afb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.744506099373989"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(embedding5, embedding6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "277f5391",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "persist_directory = 'chroma/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "167792ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!rm -rf chroma  # remove old database files if any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "9490d6da",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vectordb = Chroma.from_documents(\n",
    "    documents=splits,\n",
    "    embedding=embedding,\n",
    "    persist_directory=persist_directory\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "572d3f7b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "257\n"
     ]
    }
   ],
   "source": [
    "print(vectordb._collection.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "317f4123",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "question = \"What unlawful things did Donald Trump supposedly do?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "5e361790",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "docs = vectordb.similarity_search(question,k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "0813f474",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "1e001c7b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Please rise if you are able and show that, Yes, we the United States of America stand with the Ukrainian people. \\n\\nThroughout our history we’ve learned this lesson when dictators do not pay a price for their aggression they cause more chaos.   \\n\\nThey keep moving.   \\n\\nAnd the costs and the threats to America and the world keep rising.   \\n\\nThat’s why the NATO Alliance was created to secure peace and stability in Europe after World War 2. \\n\\nThe United States is a member along with 29 other nations. \\n\\nIt matters. American diplomacy matters. American resolve matters. \\n\\nPutin’s latest attack on Ukraine was premeditated and unprovoked. \\n\\nHe rejected repeated efforts at diplomacy. \\n\\nHe thought the West and NATO wouldn’t respond. And he thought he could divide us at home. Putin was wrong. We were ready.  Here is what we did.   \\n\\nWe prepared extensively and carefully. \\n\\nWe spent months building a coalition of other freedom-loving nations from Europe and the Americas to Asia and Africa to confront Putin. \\n\\nI spent countless hours unifying our European allies. We shared with the world in advance what we knew Putin was planning and precisely how he would try to falsely justify his aggression.  \\n\\nWe countered Russia’s lies with truth.   \\n\\nAnd now that he has acted the free world is holding him accountable.'"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[4].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "623dd2f4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vectordb.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "17feee62",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'source': 'data/doc-3.txt'}\n",
      "{'source': 'data/doc-3.txt'}\n",
      "{'source': 'data/doc-6.txt'}\n",
      "{'source': 'data/doc-3.txt'}\n",
      "{'source': 'data/state_of_the_union.txt'}\n"
     ]
    }
   ],
   "source": [
    "for doc in docs:\n",
    "    print(doc.metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "57ea535c",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = vectordb.max_marginal_relevance_search(question,k=5, fetch_k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "cec2c544",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'source': 'data/doc-3.txt'}\n",
      "{'source': 'data/doc-6.txt'}\n",
      "{'source': 'data/state_of_the_union.txt'}\n",
      "{'source': 'data/doc-4.txt'}\n",
      "{'source': 'data/state_of_the_union.txt'}\n"
     ]
    }
   ],
   "source": [
    "for doc in docs:\n",
    "    print(doc.metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "2c77edee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretty_print_docs(docs):\n",
    "    print(f\"\\n{'-' * 100}\\n\".join([f\"Document {i+1}:\\n\\n\" + d.page_content for i, d in enumerate(docs)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "db1baefb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrap our vectorstore\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "compressor = LLMChainExtractor.from_llm(llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "c82aebc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor,\n",
    "    base_retriever=vectordb.as_retriever(search_type = \"mmr\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "1a204e76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1:\n",
      "\n",
      "Don Quixote and Sancho, mounted on a donkey, set out. In their first adventure, Don Quixote mistakes a field of windmills for giants and attempts to fight them but finally concludes that a magician must have turned the giants into windmills. He later attacks a group of monks, thinking that they have imprisoned a princess, and also does battle with a herd of sheep, among other adventures, almost all of which end with Don Quixote, Sancho, or both being beaten. Eventually, Don Quixote acquires a metal washbasin from a barber, which he believes is a helmet once worn by a famous knight, and he later frees a group of convicted criminals.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 2:\n",
      "\n",
      "Don Quixote\n"
     ]
    }
   ],
   "source": [
    "compressed_docs = compression_retriever.get_relevant_documents(question)\n",
    "pretty_print_docs(compressed_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "75955b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm,\n",
    "    retriever=vectordb.as_retriever()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "251e5e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = qa_chain({\"query\": question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "4934af28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"According to the information provided, Donald Trump has been charged with several unlawful actions. These include altering, destroying, mutilating, or concealing objects and documents with classified markings, willful retention of national defense information, and conspiracy to obstruct justice. He has also been accused of instructing an employee to delete Mar-a-Lago security camera footage to prevent it from being turned over to a federal grand jury. It's important to note that these are allegations, and Trump has pleaded not guilty and denied any wrongdoing.\""
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[\"result\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "id": "ac3be5e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Build prompt\n",
    "template = \"\"\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. Use three sentences maximum. Keep the answer as concise as possible. Always say \"thanks for asking!\" at the end of the answer. \n",
    "{context}\n",
    "Question: {question}\n",
    "Helpful Answer:\"\"\"\n",
    "QA_CHAIN_PROMPT = PromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "id": "ac5baac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run chain\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm,\n",
    "    retriever=vectordb.as_retriever(),\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "id": "4a09c90b",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = qa_chain({\"query\": question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "0052fe6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Donald Trump is accused of altering, destroying, mutilating, or concealing an object and corruptly altering, destroying, mutilating, or concealing a document, record, or other object. He is also charged with willful retention of national defense information. Thanks for asking!'"
      ]
     },
     "execution_count": 308,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[\"result\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "id": "8b1dc3af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='POLITICS \\nTrump hit with new charges as special counsel expands Mar-a-Lago documents case\\nBY ROBERT LEGARE, MELISSA QUINN, KATHRYN WATSON\\n\\nUPDATED ON: JULY 28, 2023 / 3:02 AM / CBS NEWS\\n\\nWashington — Prosecutors with special counsel Jack Smith\\'s office have added new charges against former President Donald Trump in the case involving documents with classified markings discovered at this Florida resort of Mar-a-Lago, according to court papers filed in federal court Thursday evening.\\n\\nA superseding indictment unsealed by the Justice Department lists multiple new counts against Trump, including: altering, destroying, mutilating, or concealing an object; and corruptly altering, destroying, mutilating or concealing a document, record or other object; and an additional charge of willful retention of national defense information.\\n\\nTrump was previously charged with 37 felony counts, including 31 counts of willful retention of classified documents and one count of conspiracy to obstruct justice. He has pleaded not guilty and claimed the prosecution is a politically motivated \"witch hunt\" against him. Speaking Thursday with Breitbart, Trump called the charges \"harassment\" and \"election interference.\"\\n\\nWalt Nauta, the former president\\'s aide, was also charged in the case and pleaded not guilty.', metadata={'source': 'data/doc-3.txt'})"
      ]
     },
     "execution_count": 309,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[\"source_documents\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90d1919",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
