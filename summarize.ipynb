{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ffb206e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import streamlit as st\n",
    "import os\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain import PromptTemplate\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from dotenv import load_dotenv, find_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a44ea57",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(find_dotenv())\n",
    "\n",
    "open.api_key = os.environ['OPENAI_API_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b3922e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-30 12:06:19.660 WARNING streamlit.runtime.caching.cache_data_api: No runtime found, using MemoryCacheStorageManager\n"
     ]
    }
   ],
   "source": [
    "@st.cache_data\n",
    "def setup_documents(pdf_file_path, chunk_size, chunk_overlap):\n",
    "    loader = PyPDFLoader(pdf_file_path)\n",
    "    docs_raw = loader.load()\n",
    "    docs_raw_text = [doc.page_content for doc in docs_raw]\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    docs = text_splitter.create_documents(docs_raw_text)\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c77c035d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_summary(docs,llm, custom_prompt, chain_type, num_summaries):\n",
    "    custom_prompt = custom_prompt + \"\"\":\\n\\n {text}\"\"\"\n",
    "    COMBINE_PROMPT = PromptTemplate(template=custom_prompt, input_variables=[\"text\"])\n",
    "    MAP_PROMPT = PromptTemplate(template=\"Summarize:\\n\\n{text}\", input_variables=[\"text\"])\n",
    "    if chain_type == \"map_reduce\":\n",
    "        chain = load_summarize_chain(llm, chain_type=chain_type, \n",
    "                                     map_prompt=MAP_PROMPT, combine_prompt=COMBINE_PROMPT)\n",
    "    else:\n",
    "        chain = load_summarize_chain(llm, chain_type=chain_type)\n",
    "    summaries = []\n",
    "    for i in range(num_summaries):\n",
    "        summary_output = chain({\"input_documents\": docs}, return_only_outputs=True)[\"output_text\"]\n",
    "        summaries.append(summary_output)\n",
    "    \n",
    "    return summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "65aead61",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-30 12:10:15.013 WARNING streamlit.runtime.caching.cache_data_api: No runtime found, using MemoryCacheStorageManager\n"
     ]
    }
   ],
   "source": [
    "@st.cache_data\n",
    "def color_chunks(text: str, chunk_size: int, overlap_size: int) -> str:\n",
    "    overlap_color = \"#808080\" # Light gray for the overlap\n",
    "    chunk_colors = [\"#a8d08d\", \"#c6dbef\", \"#e6550d\", \"#fd8d3c\", \"#fdae6b\", \"#fdd0a2\"] # Different shades of green for chunks\n",
    "\n",
    "    colored_text = \"\"\n",
    "    overlap = \"\"\n",
    "    color_index = 0\n",
    "\n",
    "    for i in range(0, len(text), chunk_size-overlap_size):\n",
    "        chunk = text[i:i+chunk_size]\n",
    "        if overlap:\n",
    "            colored_text += f'<mark style=\"background-color: {overlap_color};\">{overlap}</mark>'\n",
    "        chunk = chunk[len(overlap):]\n",
    "        colored_text += f'<mark style=\"background-color: {chunk_colors[color_index]};\">{chunk}</mark>'\n",
    "        color_index = (color_index + 1) % len(chunk_colors)\n",
    "        overlap = text[i+chunk_size-overlap_size:i+chunk_size]\n",
    "\n",
    "    return colored_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e641fdeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    st.set_page_config(layout=\"wide\")\n",
    "    st.title(\"Custom Summarization App\")\n",
    "    chain_type = st.sidebar.selectbox(\"Chain Type\", [\"map_reduce\", \"stuff\", \"refine\"])\n",
    "    chunk_size = st.sidebar.slider(\"Chunk Size\", min_value=100, max_value=10000, step=100, value=1900)\n",
    "    chunk_overlap = st.sidebar.slider(\"Chunk Overlap\", min_value=100, max_value=10000, step=100, value=200)\n",
    "    \n",
    "    if st.sidebar.checkbox(\"Debug chunk size\"):\n",
    "        st.header(\"Interactive Text Chunk Visualization\")\n",
    "\n",
    "        text_input = st.text_area(\"Input Text\", \"This is a test text to showcase the functionality of the interactive text chunk visualizer.\")\n",
    "\n",
    "        # Set the minimum to 1, the maximum to 5000 and default to 100\n",
    "        html_code = color_chunks(text_input, chunk_size, chunk_overlap)\n",
    "        st.markdown(html_code, unsafe_allow_html=True)\n",
    "    \n",
    "    else:\n",
    "        user_prompt = st.text_input(\"Enter the user prompt\")\n",
    "        pdf_file_path = st.text_input(\"Enter the pdf file path\")\n",
    "        \n",
    "        temperature = st.sidebar.number_input(\"ChatGPT Temperature\", min_value=0.0, max_value=1.0, step=0.1, value=0.0)\n",
    "        num_summaries = st.sidebar.number_input(\"Number of Summaries\", min_value=1, max_value=10, step=1, value=1)\n",
    "        \n",
    "        # make the choice of llm to select from a selectbox\n",
    "        llm = st.sidebar.selectbox(\"LLM\", [\"ChatGPT\", \"GPT4\", \"\"])\n",
    "        if llm == \"ChatGPT\":\n",
    "            llm = ChatOpenAI(temperature=temperature)\n",
    "        elif llm == \"GPT4\":\n",
    "            llm = ChatOpenAI(model_name=\"gpt-4\",temperature=temperature)\n",
    "        \n",
    "        if pdf_file_path != \"\":\n",
    "            docs = setup_documents(pdf_file_path, chunk_size, chunk_overlap)\n",
    "            st.write(\"Pdf was loaded successfully\")\n",
    "            \n",
    "            if st.button(\"Summarize\"):\n",
    "                result = custom_summary(docs,llm, user_prompt, chain_type, num_summaries)\n",
    "                st.write(\"Summaries:\")\n",
    "                for summary in result:\n",
    "                    st.write(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "00f6da95",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-30 12:11:34.761 \n",
      "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
      "  command:\n",
      "\n",
      "    streamlit run /usr/local/lib/python3.10/dist-packages/ipykernel_launcher.py [ARGUMENTS]\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab2afbe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
