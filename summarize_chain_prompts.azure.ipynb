{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c158454d",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This notebook explores summarization templates and the default prompts supplied by LangChain.\n",
    "\n",
    "It uses the Azure OpenAI models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "007386fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import textwrap\n",
    "\n",
    "from helpers.utilities import install_if_needed, load_keys, get_env_file_keys\n",
    "from dotenv import find_dotenv, load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "492a3953",
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV_FILE_NAME = '.env_azure_openai'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f5247f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['OPENAI_API_TYPE', 'OPENAI_API_VERSION', 'OPENAI_API_BASE', 'OPENAI_API_KEY']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_env_file_keys(ENV_FILE_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b5b764f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_keys(ENV_FILE_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "28807243",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "langchain is already installed.\n"
     ]
    }
   ],
   "source": [
    "install_if_needed (['langchain'])\n",
    "\n",
    "from langchain.chat_models import AzureChatOpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f51f4457",
   "metadata": {},
   "source": [
    "## load_summarize_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fd32c057",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.document_loaders import WebBaseLoader\n",
    "\n",
    "loader = WebBaseLoader(\"https://lilianweng.github.io/posts/2023-06-23-agent/\")\n",
    "docs = loader.load()\n",
    "\n",
    "llm = AzureChatOpenAI(deployment_name=\"gpt-35-turbo-16k\",\n",
    "                      temperature=0,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b551fb72",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = load_summarize_chain(llm, chain_type=\"stuff\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6f1c2c25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 63 ms, sys: 4.45 ms, total: 67.5 ms\n",
      "Wall time: 47.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "result = chain.run(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c086b193",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The article discusses the concept of building autonomous agents\n",
      "powered by large language models (LLMs). It explores the components of\n",
      "such agents, including planning, memory, and tool use. The article\n",
      "provides case studies and proof-of-concept examples of LLM-powered\n",
      "agents in various domains. It also highlights the challenges and\n",
      "limitations of using LLMs in agent systems.\n"
     ]
    }
   ],
   "source": [
    "print(textwrap.fill(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "72d3583f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['memory', 'callbacks', 'callback_manager', 'verbose', 'tags', 'metadata', 'input_key', 'output_key', 'llm_chain', 'document_prompt', 'document_variable_name', 'document_separator'])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.__fields__.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "deadc10e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write a concise summary of the following:\n",
      "\n",
      "\n",
      "\"{text}\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\n"
     ]
    }
   ],
   "source": [
    "print(chain.llm_chain.prompt.template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b9db0c",
   "metadata": {},
   "source": [
    "## StuffDocumentsChain\n",
    "\n",
    "The load_summarize_chain with chain_type stuff is really just an instance of a StuffDocumentsChain built from a PromptTemplate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "460703b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.llm import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains.combine_documents.stuff import StuffDocumentsChain\n",
    "\n",
    "prompt_template = \"\"\"Write a concise summary of the following:\n",
    "\"{text}\"\n",
    "CONCISE SUMMARY:\"\"\"\n",
    "prompt = PromptTemplate.from_template(prompt_template)\n",
    "\n",
    "# Define LLM chain\n",
    "llm = llm\n",
    "llm_chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "stuff_chain = StuffDocumentsChain(llm_chain=llm_chain, document_variable_name=\"text\")\n",
    "\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aaf1cb9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 57.4 ms, sys: 2.13 ms, total: 59.5 ms\n",
      "Wall time: 5.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "result = stuff_chain.run(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "29940781",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The article discusses the concept of building autonomous agents\n",
      "powered by large language models (LLMs). It explores the components of\n",
      "such agents, including planning, memory, and tool use. The article\n",
      "provides case studies and examples of proof-of-concept demos,\n",
      "highlighting the challenges and limitations of LLM-powered agents. It\n",
      "also includes references to related research papers and provides a\n",
      "citation for the article.\n"
     ]
    }
   ],
   "source": [
    "print(textwrap.fill(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3e94e2f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write a concise summary of the following:\n",
      "\"{text}\"\n",
      "CONCISE SUMMARY:\n"
     ]
    }
   ],
   "source": [
    "print(stuff_chain.llm_chain.prompt.template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd2475d",
   "metadata": {},
   "source": [
    "## Map-Reduce\n",
    "\n",
    "Now we explore the use of map-reduce to address inputs that are too large."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "40352739",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Switching to gpt-35-turbo which has a smaller input window than gpt-3.5-turbo-16k.\n",
    "llm = AzureChatOpenAI(deployment_name=\"gpt-35-turbo\",\n",
    "                      temperature=0,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "920d60d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the stuff chain around the smaller input window llm.\n",
    "llm_chain = LLMChain(llm=llm, prompt=prompt)\n",
    "smaller_stuff_chain = StuffDocumentsChain(llm_chain=llm_chain, document_variable_name=\"text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1d1da966",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caught an Invalid Request Error: This model's maximum context length is 4096 tokens. However, your messages resulted in 9530 tokens. Please reduce the length of the messages.\n"
     ]
    }
   ],
   "source": [
    "# Generate the anticipated error.\n",
    "try:\n",
    "    smaller_stuff_chain.run(docs)\n",
    "except Exception as e:\n",
    "    print(f\"Caught an Invalid Request Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1f6c6842",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.mapreduce import MapReduceChain\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chains import MapReduceDocumentsChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fee051d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=5000, chunk_overlap=200)\n",
    "chunks = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "956387f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f9d599eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "map_reduce_chain = load_summarize_chain(llm, chain_type=\"map_reduce\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "75b9b898",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised APIConnectionError: Error communicating with OpenAI: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response')).\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised ServiceUnavailableError: The server is overloaded or not ready yet..\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised APIConnectionError: Error communicating with OpenAI: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response')).\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised APIConnectionError: Error communicating with OpenAI: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response')).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 514 ms, sys: 47.8 ms, total: 562 ms\n",
      "Wall time: 15min 21s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "result = map_reduce_chain.run(input_documents=chunks, return_only_outputs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9e12763b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This article discusses the concept of building autonomous agents\n",
      "powered by large language models (LLMs) and explores the components of\n",
      "such agents, including planning, memory, and tool use. It also\n",
      "discusses frameworks for improving reasoning skills in AI agents and\n",
      "explores different types of human memory. The article mentions various\n",
      "algorithms and architectures used in similarity search and tool use in\n",
      "language models. It discusses the HuggingGPT system and its\n",
      "challenges, as well as case studies in the ChemCrow domain, scientific\n",
      "discovery, and generative agents. The article also describes a\n",
      "generative agent architecture and a proof-of-concept example called\n",
      "AutoGPT. It provides information on the response format for the GPT-\n",
      "Engineer project and includes a sample conversation between the user\n",
      "and the assistant. The challenges of limited context length and long-\n",
      "term planning in building LLM-centered agents are mentioned, as well\n",
      "as the reliability issues of natural language interfaces in current\n",
      "agent systems.\n"
     ]
    }
   ],
   "source": [
    "print(textwrap.fill(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8969b029",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['memory', 'callbacks', 'callback_manager', 'verbose', 'tags', 'metadata', 'input_key', 'output_key', 'llm_chain', 'reduce_documents_chain', 'document_variable_name', 'return_intermediate_steps'])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "map_reduce_chain.__fields__.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e12340f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write a concise summary of the following:\n",
      "\n",
      "\n",
      "\"{text}\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\n"
     ]
    }
   ],
   "source": [
    "print(map_reduce_chain.llm_chain.prompt.template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0073893c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write a concise summary of the following:\n",
      "\n",
      "\n",
      "\"{text}\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\n"
     ]
    }
   ],
   "source": [
    "print(map_reduce_chain.reduce_documents_chain.combine_documents_chain.llm_chain.prompt.template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da9c04bf",
   "metadata": {},
   "source": [
    "## Map-Refine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7d3ac037",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RefineDocumentsChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b0d5a86b",
   "metadata": {},
   "outputs": [],
   "source": [
    "refine_chain = load_summarize_chain(llm, chain_type=\"refine\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a071a3dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised ServiceUnavailableError: The server is overloaded or not ready yet..\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised ServiceUnavailableError: The server is overloaded or not ready yet..\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised ServiceUnavailableError: The server is overloaded or not ready yet..\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised ServiceUnavailableError: The server is overloaded or not ready yet..\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised ServiceUnavailableError: The server is overloaded or not ready yet..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 186 ms, sys: 21.3 ms, total: 208 ms\n",
      "Wall time: 4min 2s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "result = refine_chain.run(input_documents=chunks, return_only_outputs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ca0e26d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The original summary is still relevant and does not need to be\n",
      "refined.\n"
     ]
    }
   ],
   "source": [
    "print(textwrap.fill(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a2d2ca4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['memory', 'callbacks', 'callback_manager', 'verbose', 'tags', 'metadata', 'input_key', 'output_key', 'initial_llm_chain', 'refine_llm_chain', 'document_variable_name', 'initial_response_name', 'document_prompt', 'return_intermediate_steps'])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "refine_chain.__fields__.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3efd0a28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write a concise summary of the following:\n",
      "\n",
      "\n",
      "\"{text}\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\n"
     ]
    }
   ],
   "source": [
    "print(refine_chain.initial_llm_chain.prompt.template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "740d0672",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your job is to produce a final summary\n",
      "We have provided an existing summary up to a certain point: {existing_answer}\n",
      "We have the opportunity to refine the existing summary(only if needed) with some more context below.\n",
      "------------\n",
      "{text}\n",
      "------------\n",
      "Given the new context, refine the original summary\n",
      "If the context isn't useful, return the original summary.\n"
     ]
    }
   ],
   "source": [
    "print(refine_chain.refine_llm_chain.prompt.template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "378b1f87",
   "metadata": {},
   "source": [
    "The refine chain doesn't seem to be working exactly as anticipated.  The output seems to relate to the impact of the last chunk and doesn't reflect the aggregated response.  Perhaps I am looking in the wrong part of the object.  Let's repeat but change the return_only_outputs parameter to False and then look more carefully at the result object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5395b1b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised APIConnectionError: Error communicating with OpenAI: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response')).\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised ServiceUnavailableError: The server is overloaded or not ready yet..\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised ServiceUnavailableError: The server is overloaded or not ready yet..\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised ServiceUnavailableError: The server is overloaded or not ready yet..\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised ServiceUnavailableError: The server is overloaded or not ready yet..\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised ServiceUnavailableError: The server is overloaded or not ready yet..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 265 ms, sys: 21.7 ms, total: 287 ms\n",
      "Wall time: 8min 2s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "result = refine_chain.run(input_documents=chunks, return_only_outputs=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "750d35d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The original summary is still relevant and does not need to be refined.'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
